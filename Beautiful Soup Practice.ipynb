{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f5ff77",
   "metadata": {},
   "source": [
    "1) Write a python program to display all the header tags from wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f013a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5376127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the Wikipedia homepage\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d32409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sathp\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'en.wikipedia.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Send a GET request to the URL without verifying SSL certificate\n",
    "response = requests.get(url, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7828c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb0c7b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all header tags\n",
    "headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aff23fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Page\n",
      "Welcome to Wikipedia\n",
      "From today's featured article\n",
      "Did you know ...\n",
      "In the news\n",
      "On this day\n",
      "Today's featured picture\n",
      "Other areas of Wikipedia\n",
      "Wikipedia's sister projects\n",
      "Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "# Display the text content of each header tag\n",
    "for header in headers:\n",
    "    print(header.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30c86e",
   "metadata": {},
   "source": [
    "2. Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b872e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc937ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of IMDb's Top Rated Movies page\n",
    "url = \"https://www.imdb.com/chart/top/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4e3abae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sathp\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.imdb.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Send a GET request to the URL without verifying SSL certificate\n",
    "response = requests.get(url, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1f400cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f4f75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the list of top-rated movies\n",
    "movies = soup.find_all('td', class_='titleColumn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b6accb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store movie data\n",
    "names = []\n",
    "ratings = []\n",
    "years = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "503a2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from each movie\n",
    "for movie in movies[:100]:  # Limiting to top 100 movies\n",
    "    name = movie.find('a').text\n",
    "    rating = movie.find_next_sibling('td').find('strong').text\n",
    "    year = movie.find('span').text.strip('()')\n",
    "    names.append(name)\n",
    "    ratings.append(float(rating))\n",
    "    years.append(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "532eabd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Name': names, 'Rating': ratings, 'Year': years})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0cb460c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Rating, Year]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d9e82a",
   "metadata": {},
   "source": [
    "3. Write a python program to scrape mentioned details from dineout.co.in : i) Restaurant name ii) Cuisine iii) Location iv) Ratings v) Image URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bac8f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8972748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the page to scrape\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "688c2d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sathp\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.dineout.co.in'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Send a GET request to the URL without verifying SSL certificate\n",
    "response = requests.get(url, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20ffa26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b582619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all restaurant items\n",
    "restaurants = soup.find_all('div', class_='restaurant-detail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80062699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store restaurant details\n",
    "restaurant_names = []\n",
    "cuisines = []\n",
    "locations = []\n",
    "ratings = []\n",
    "image_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00c9c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from each restaurant\n",
    "for restaurant in restaurants:\n",
    "    # Restaurant name\n",
    "    name = restaurant.find('div', class_='restnt-info cursor')\\\n",
    "                     .find('div', class_='restnt-info-main')\\\n",
    "                     .find('h3')\\\n",
    "                     .text.strip()\n",
    "    restaurant_names.append(name)\n",
    "\n",
    "    # Cuisine\n",
    "    cuisine = restaurant.find('div', class_='restnt-info cursor')\\\n",
    "                        .find('div', class_='restnt-info-main')\\\n",
    "                        .find('span', class_='double-line-ellipsis')\\\n",
    "                        .text.strip()\n",
    "    cuisines.append(cuisine)\n",
    "\n",
    "    # Location\n",
    "    location = restaurant.find('div', class_='restnt-info cursor')\\\n",
    "                         .find('div', class_='restnt-info-main')\\\n",
    "                         .find('p', class_='restnt-loc ellipsis')\\\n",
    "                         .text.strip()\n",
    "    locations.append(location)\n",
    "\n",
    "    # Ratings\n",
    "    rating = restaurant.find('div', class_='restnt-info cursor')\\\n",
    "                       .find('div', class_='rating-score')\\\n",
    "                       .text.strip()\n",
    "    ratings.append(rating)\n",
    "\n",
    "    # Image URL\n",
    "    image_url = restaurant.find('div', class_='restnt-photo')\\\n",
    "                         .find('img')['data-src']\n",
    "    image_urls.append(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb5680e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the scraped data\n",
    "for i in range(len(restaurant_names)):\n",
    "    print(\"Restaurant Name:\", restaurant_names[i])\n",
    "    print(\"Cuisine:\", cuisines[i])\n",
    "    print(\"Location:\", locations[i])\n",
    "    print(\"Ratings:\", ratings[i])\n",
    "    print(\"Image URL:\", image_urls[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce0308",
   "metadata": {},
   "source": [
    "4. Write s python program to display list of respected former finance minister of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eac25d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de9812cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the website to scrape\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7937381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a GET request to the URL\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "333aa0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1869d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the table containing the list of former finance ministers\n",
    "table = soup.find('table', class_='tableizer-table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43809c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table not found.\n"
     ]
    }
   ],
   "source": [
    "# Check if the table is found\n",
    "if table is not None:\n",
    "    # Initialize empty lists to store data\n",
    "    names = []\n",
    "    terms_of_office = []\n",
    "\n",
    "    # Extract data from the table rows\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        name = cols[0].text.strip()\n",
    "        term_of_office = cols[1].text.strip()\n",
    "        names.append(name)\n",
    "        terms_of_office.append(term_of_office)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({'Name': names, 'Term of Office': terms_of_office})\n",
    "\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Table not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
